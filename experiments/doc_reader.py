# -*- coding: utf-8 -*-
"""3 - Document Reader Experiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QihqWyfNtOc-N0M6wW7s9V_9sEiEuzMv

# Document Reader Experiments

In Question Answering (QA), queries are run over several documents to extract an answer to user questions, consisting of two main steps: (1) Document Retriever — retrieve the most useful documents that may contain the answer to a given question; (2) Document Reader — a machine reader carefully examines the retrieved documents and frame an answer.

In this Jupyter Notebook, we focused on the Document Reader experiments, motivated by the fact that using a good Reader (higher F1) produces a better and more concise response.

---
## Setup

Packages installation and setups.

### Run Configuration

Choose the dataset and the Document Reader algorithm.
"""

import enum


class Dataset(enum.Enum):
    """Dataset options"""

    SQuAD = 1
    AdvQA = 2
    DuoRC = 3
    QASports = 4


class DocReader:
    """Document Reader options"""

    BERT = "deepset/bert-base-uncased-squad2"
    RoBERTa = "deepset/roberta-base-squad2"
    MiniLM = "deepset/minilm-uncased-squad2"
    DistilBERT = "distilbert-base-uncased-distilled-squad"
    FineDistilBERT = "laurafcamargos/distilbert-qasports-basket-small"
    ELECTRA = "deepset/electra-base-squad2"


class Sports:
    BASKETBALL = "basketball"
    FOOTBALL = "football"
    SOCCER = "soccer"
    ALL = "all"


# basketball
# football
# americanfootball
# cricket
# hockey
# golf
# rugbyunion
# rugby
# baseball
# martialarts
# boxing
# mixedmartialarts
# fitness
# cycling
# badminton
# gymnastics
# handball
# skiing
# horse_racing
# f1

# run configuration
NUM_K = 1  # always = 1
DATASET = Dataset.QASports
DOC_READER = DocReader.DistilBERT
SPORT = Sports.ALL

"""### Package Installation

Install Haystack and HuggingFace packages.
"""

# Check if you have a GPU running
# The code runs in CPU as well
# !nvidia-smi


"""### Logging

We configure how logging messages should be displayed and which log level should be used before importing Haystack.
"""

import logging

# Setup Haystack logging format
logging.basicConfig(format="%(levelname)s - %(message)s", level=logging.WARNING)
logging.getLogger("haystack").setLevel(logging.INFO)

"""---
## Dataset

Importing and download the respective dataset.

### Abstract Dataset
"""

import pandas as pd
from abc import ABCMeta, abstractmethod


class AbstactDataset(metaclass=ABCMeta):
    """Abstract dataset class"""

    def __init__(self):
        self.raw_dataset = self.download()
        self.df_dataset = self._transform_df()
        print(f"## {self.name} ##")
        print(self.raw_dataset)

    def _transform_df(self):
        """Transform dataset in a pd.DataFrame"""
        return pd.DataFrame(self.raw_dataset)

    @property
    @abstractmethod
    def name(self):
        """Dataset name"""
        pass

    @abstractmethod
    def download(self):
        """Download the dataset"""
        pass

    @abstractmethod
    def get_documents(self):
        """Get the unique documents to store in the Document Store"""
        pass

    @abstractmethod
    def get_validation(self):
        """Get the validation set"""
        pass


"""### SQuaD Dataset

https://huggingface.co/datasets/squad
"""

import mmh3
from datasets import load_dataset
from haystack.schema import Label, Document, Answer
from haystack.schema import EvaluationResult, MultiLabel


class SQuadDataset(AbstactDataset):
    """SQuaD Dataset"""

    name = "SQuaD Dataset"
    _columns = {
        "title": "title",
        "document": "context",
        "question": "question",
    }
    _metadata = {"dataset_id": "id"}

    def download(self):
        dataset = load_dataset("squad", split="validation")
        return dataset

    def get_documents(self):
        # Remove duplicated contents
        cc = self._columns
        dataset_name = f"{self.name}"
        df = self.df_dataset
        df = df.drop_duplicates(subset=[cc["title"], cc["document"]], keep="first")

        # Create Haystack Document objects
        list_docs = []
        for _, row in df.iterrows():
            document_id = mmh3.hash128(row[cc["document"]], signed=False)
            doc_metadata = {k: row[v] for k, v in self._metadata.items()}
            doc_metadata["title"] = row[cc["title"]]
            doc_metadata["dataset_name"] = dataset_name
            doc = Document(
                id=document_id,
                content_type="text",
                content=row[cc["document"]],
                meta=doc_metadata,
            )
            list_docs.append(doc)
        return list_docs

    def _get_answers(self, data):
        # Get question answer
        return data["answers"]["text"]

    def get_validation(self):
        # Get dataset info
        cc = self._columns
        df = self.df_dataset
        _self = self

        # Create Haystack labels
        eval_labels = []
        for _, row in df.iterrows():
            document_id = mmh3.hash128(row[cc["document"]], signed=False)
            doc_label = MultiLabel(
                labels=[
                    Label(
                        query=row[cc["question"]],
                        answer=Answer(answer=answer, type="extractive"),
                        document=Document(
                            id=document_id,
                            content_type="text",
                            content=row[cc["document"]],
                        ),
                        is_correct_answer=True,
                        is_correct_document=True,
                        origin="gold-label",
                    )
                    for answer in _self._get_answers(row)
                ]
            )
            eval_labels.append(doc_label)
        return eval_labels


"""### AdversarialQA Dataset

https://huggingface.co/datasets/adversarial_qa
"""


class AdversarialQADataset(SQuadDataset):
    """AdversarialQA Dataset"""

    name = "AdversarialQA Dataset"

    def download(self):
        dataset = load_dataset("adversarial_qa", "adversarialQA", split="validation")
        return dataset


"""### DuoRC Dataset

https://huggingface.co/datasets/duorc
"""


class DuoRCDataset(SQuadDataset):
    """DuoRC  Dataset"""

    name = "DuoRC Dataset"
    _columns = {
        "title": "title",
        "document": "plot",
        "question": "question",
    }
    _metadata = {"dataset_id": "question_id"}

    def download(self):
        dataset = load_dataset("duorc", "SelfRC", split="validation")
        return dataset

    def _transform_df(self):
        """Transform dataset in a pd.DataFrame"""
        df = pd.DataFrame(self.raw_dataset)
        # Get questions with answer
        return df[~df["no_answer"]]

    def _get_answers(self, data):
        # Get question answer
        return data["answers"]


"""### QASports Dataset

https://huggingface.co/datasets/PedroCJardim/QASports
"""


class QASportsDataset(SQuadDataset):
    """QASports  Dataset"""

    name = "QASports Dataset"
    _columns = {
        "title": "context_title",
        "document": "context",
        "question": "question",
    }
    _metadata = {"dataset_id": "id_qa"}

    def __init__(self, sport=None):
        self.sport = sport
        super().__init__()

    def download(self):
        dataset = load_dataset("PedroCJardim/QASports", self.sport, split="validation")
        return dataset

    def _transform_df(self):
        """Transform dataset in a pd.DataFrame"""
        df = pd.DataFrame(self.raw_dataset)
        # Get questions with answer
        df["answer"] = df["answer"].apply(eval)
        mask = df["answer"].apply(lambda x: True if x["text"] != "" else False)
        return df[mask]

    def _get_answers(self, data):
        # Get question answer
        return [data["answer"]["text"]]


"""### Download the dataset

Get the dataset and store the documents in the Document Store.
"""


def dataset_switch(choice):
    """Get dataset class"""

    if choice == Dataset.SQuAD:
        return SQuadDataset()
    elif choice == Dataset.AdvQA:
        return AdversarialQADataset()
    elif choice == Dataset.DuoRC:
        return DuoRCDataset()
    elif choice == Dataset.QASports:
        return QASportsDataset(SPORT)
    else:
        return "Invalid dataset"


# Get the dataset
dataset = dataset_switch(DATASET)
docs = dataset.get_documents()

"""---
## Document Reader

In this experiment, we explored three Transformer based models for extractive Question Answering using the [FARM framework](https://github.com/deepset-ai/FARM).
* [BERT paper](https://arxiv.org/abs/1810.04805), [implementation](https://huggingface.co/deepset/bert-base-uncased-squad2)
* [RoBERTa paper](https://arxiv.org/abs/1907.11692), [implementation](https://huggingface.co/deepset/roberta-base-squad2)
* [MiniLM paper](https://arxiv.org/abs/2002.10957), [implementation](https://huggingface.co/deepset/minilm-uncased-squad2)

"""

from haystack.nodes import FARMReader

# Get the reader
reader = FARMReader(DOC_READER, use_gpu=True)

from haystack import Pipeline

# Build the pipeline
pipe = Pipeline()
pipe.add_node(component=reader, name="Reader", inputs=["Query"])

# Testing the pipeline
from haystack.utils import print_answers

# Querying documents
question = "Who did the Raptors face in the first round of the 2015 Playoffs?"
prediction = pipe.run(
    query=question, documents=docs[0:10], params={"Reader": {"top_k": 3}}
)

# Print answer
print_answers(prediction)

"""---
## Evaluation

About the metrics, you can read the [evaluation](https://docs.haystack.deepset.ai/docs/evaluation) web page.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
#
# For testing purposes, running on the first 100 labels
# For real evaluation, you must remove the [0:100]
eval_labels = dataset.get_validation()
eval_docs = [
    [label.document for label in multi_label.labels] for multi_label in eval_labels
]

eval_result = pipe.eval(
    labels=eval_labels, documents=eval_docs, params={"Reader": {"top_k": NUM_K}}
)

from pprint import pprint

# Get and print the metrics
metrics = eval_result.calculate_metrics()
pprint(metrics)

# Print a detailed report
# pipe.print_eval_report(eval_result)
