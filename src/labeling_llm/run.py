# -*- coding: utf-8 -*-
"""LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YTQIPaRNt1W63aB5QTROJB4y0jDQ0wyy
"""

#               IMPORTS
import ast
import json
import re
import torch
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM



#               CONFIGURATION
CSV_PATH   = "/content/golf-qa-sampling.csv"         # Path to the CSV to evaluate
MODEL_NAME = "Qwen/Qwen2.5-3B-Instruct"              # Choose one model:
# MODEL_OPTIONS = [
#     "Qwen/Qwen2.5-3B-Instruct",
#     "Qwen/Qwen2.5-7B-Instruct",
#     "Qwen/Qwen2.5-14B-Instruct",
#     "Qwen/Qwen2.5-72B-Instruct"



#               MODEL LOADING

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)




model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    trust_remote_code=True
).eval()




#               FUNCTIONS


def is_question_valid(question: str, context: str, answer: str) -> str:
    """
    Evaluates whether a question makes sense within the given context.

    Parameters:
    - question (str): The question to validate.
    - context (str): The context of the question.
    - answer (str): The answer provided for that question.

    Returns:
    - str: "1" if valid, "2" if the subject is not covered in the context.
    """
    prompt = f"""
You are an evaluator. Given a context, a question, and an answer, classify the question by selecting one of the options below.

Context: {context}
Question: {question}
Answer: {answer}

Does the question make sense based on the context?
1 - Yes
2 - No, the subject is not in the context

Respond with only the number (1 or 2). Do not include explanations.
"""

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=10)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)


    return answer.split("explanations.")[-1].strip()


def is_answer_correct(question: str, context: str, answer: str) -> str:
    """
    Evaluates whether the provided answer is correct according to the context.

    Parameters:
    - question (str): The question being asked.
    - context (str): The context in which to evaluate the answer.
    - answer (str): The answer to verify.

    Returns:
    - str: "1" if correct, "2" if incorrect but the answer is found in the context.
    """
    prompt = f"""
You are an evaluator. Given a context, a question, and a provided answer, classify the answer selecting one option.

Context: {context}
Question: {question}
Provided Answer: {answer}

Is the answer correct?
1 - Yes.
2 - No, and the answer is in the context

Return only the number (1 or 2). Do not include any explanation.
"""

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=10)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)


    return answer.split("any explanation.")[-1].strip()

#           LOAD DATAFRAME
df = pd.read_csv(CSV_PATH, on_bad_lines="skip")
df["answer_dict"] = df["answer"].apply(ast.literal_eval)



#           EVALUATION LOOP
results = []

for _, row in df.iterrows():  # Use .head(N) to test with a smaller sample
    question = row["question"]
    context = row["context"]
    answer_text = row["answer_dict"]["text"]

    valid_q      = is_question_valid(question, context, answer_text)
    correct_a    = is_answer_correct(question, context, answer_text)

    results.append({
        "context": context,
        "question": question,
        "answer": answer_text,
        "question_valid": valid_q,
        "answer_correct": correct_a,
    })

print("Evaluation completed:", len(results))

with open("/content/avaliacoes.json", "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=4)

print("Results saved to /content/evaluations.json")